{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17GWLNju7k-MaqNGCQVvA_m4g_d0_v35D?usp=sharing)\n"
      ],
      "metadata": {
        "id": "zce0NjO_HUW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "pj3m6jF9wgVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else None"
      ],
      "metadata": {
        "id": "SLkU2Wcqw9Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAGKAj_RxEvw",
        "outputId": "fba0ba03-9c5e-467e-f822-59501ba2f397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quadro RTX 6000'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BTsUxrL5CjS",
        "outputId": "8b6fa048-7652-4647-be72-79f7ee5cc552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Sat May 27 23:26:40 2023       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 515.57.02    Driver Version: 516.93       CUDA Version: 11.7     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:0B:00.0 Off |                  N/A |\n",
            "| 35%   56C    P0   112W / 350W |     14MiB / 12288MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  Quadro RTX 6000     On   | 00000000:43:00.0 Off |                  Off |\n",
            "| 41%   65C    P2    73W / 260W |  21341MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A        22      G   /Xwayland                       N/A      |\n",
            "|    0   N/A  N/A      2792      C   /python3.9                      N/A      |\n",
            "|    1   N/A  N/A        22      G   /Xwayland                       N/A      |\n",
            "|    1   N/A  N/A      2792      C   /python3.9                      N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir image2music \n",
        "%cd image2music"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4gJ9YIKxtQa",
        "outputId": "6006621b-6561-41e5-cf83-ba1403f827c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/revolt/image2music_min/image2music\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('AnyaSchen/image2music_abc')"
      ],
      "metadata": {
        "id": "g5GgKqFiVEDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a933e5-4ff1-4c4c-a2fc-62274ea7dcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found cached dataset parquet (/home/revolt/.cache/huggingface/datasets/AnyaSchen___parquet/AnyaSchen--image2music_abc-784eee9f15716c2e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 398.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import ViTFeatureExtractor, AutoTokenizer\n",
        "\n",
        "class ImageMusicDataset(Dataset):\n",
        "    def __init__(self, dataset, vit_feature_extractor, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.vit_feature_extractor = vit_feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load and preprocess the image\n",
        "        image = self.dataset[idx]['image'].convert(\"RGB\")\n",
        "        inputs = self.vit_feature_extractor(images=image, return_tensors=\"pt\", padding=True)\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "\n",
        "        # Get the ABC notation text\n",
        "        text = f\"<bos>{self.dataset[idx]['music']}<eos>\"\n",
        "\n",
        "        # Tokenize the ABC notation text\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=450, truncation=True)\n",
        "        input_ids = tokens[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = tokens[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            'labels': input_ids.clone()\n",
        "        }\n",
        "\n",
        "# Load the ViT feature extractor\n",
        "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Load a BART tokenizer for the ABC notation\n",
        "tokenizer = AutoTokenizer.from_pretrained('sander-wood/text-to-music')\n",
        "SPECIAL_TOKENS = {'bos_token':'<bos>','eos_token' :'<eos>', 'pad_token':'<pad>', 'sep_token': '<sep>'}\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
        "\n",
        "# Create the Dataset\n",
        "dataset = ImageMusicDataset(data['train'], vit_feature_extractor, tokenizer)\n",
        "\n",
        "# Example usage\n",
        "sample = dataset[0]\n",
        "print(sample[\"pixel_values\"].shape)  # Processed image tensor\n",
        "print(sample[\"input_ids\"].shape)  # Tokenized music input IDs\n",
        "print(sample[\"attention_mask\"].shape)  # Tokenized music attention mask"
      ],
      "metadata": {
        "id": "Wwz2Yz4BHHCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5ff744-51c6-4520-87b5-20351b679b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch.Size([450])\n",
            "torch.Size([450])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSpOW2dnHBZr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "606628c7-6a9e-4f9c-de78-4bc4ef8a5a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at sander-wood/text-to-music were not used when initializing BartForCausalLM: ['model.encoder.layers.1.fc1.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.shared.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'final_logits_bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.0.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/600 2:35:41, Epoch 125/150]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.246109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.143557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.713250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.468160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.329846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.240067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.135495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.048815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.970275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.896106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.841901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.779040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.717214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.657271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.600182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.556965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.505228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.455978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.408659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.362915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.327350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.284231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.242288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.201024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.160734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.129634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.091568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.055210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.019957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.986371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.960751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.930670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.902415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.876390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.852309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.835122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.816364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.799247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.784644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.772081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.763343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.753980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.746652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.739951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.735099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.731110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.728028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.724543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.722788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.720609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.719886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.719256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.716809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.716606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.716362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.716347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.716674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.719735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.719823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.719664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.720643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.721004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.721721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.721978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.722454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.722550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.723216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.723471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.724754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.724944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.724437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.725507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.727022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.726622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.726661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.727933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.728427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.729199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.729797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.730463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.730593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.731705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.731605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.732460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.732738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.732439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.733614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.734561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.734118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.734017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.734585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.735878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.736286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.736205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.736321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.737114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.737723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.737708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.736949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.737048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.737419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.737953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.066400</td>\n",
              "      <td>0.738568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vit_feature_extractor/preprocessor_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig, TrainingArguments, Trainer\n",
        "\n",
        "# Define the encoder and decoder names\n",
        "encoder_name = 'google/vit-base-patch16-224-in21k'\n",
        "decoder_name = 'sander-wood/text-to-music'\n",
        "\n",
        "# Create a configuration for VisionEncoderDecoderModel\n",
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    encoder_name, decoder_name)\n",
        "\n",
        "model.to(device)\n",
        "model.decoder.resize_token_embeddings(len(tokenizer))\n",
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Split the dataset into train and validation sets (80-20 split)\n",
        "train_size = int(0.95 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 20\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=150,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps = 10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./image_music_logs\",\n",
        "    save_steps = 1000,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    # fp16=True,  # Use mixed precision training if possible (requires an NVIDIA GPU with Tensor Cores)\n",
        ")\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('./model')\n",
        "tokenizer.save_pretrained('./tokenizer')\n",
        "vit_feature_extractor.save_pretrained('./vit_feature_extractor')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load to huging face"
      ],
      "metadata": {
        "id": "SjWpdMnuT1-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!huggingface-cli login --token {auth_token}"
      ],
      "metadata": {
        "id": "cyUbSs4NTuyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2cc0fe-5a66-4075-f0f5-9aea249eefbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (0.14.1)\n",
            "Requirement already satisfied: filelock in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: fsspec in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (2023.5.0)\n",
            "Requirement already satisfied: requests in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (2.29.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from huggingface_hub) (23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid.\n",
            "Your token has been saved to /home/revolt/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'AnyaSchen/image2music'\n",
        "fine_tuned_model.push_to_hub(path)\n",
        "tokenizer.push_to_hub(path)\n",
        "feature_extractor.push_to_hub(path)"
      ],
      "metadata": {
        "id": "Ou2ynJGjTMnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "E-ZPf7SyHh-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "yjb1PR_qHj8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if toch.cuda.is_available() else None"
      ],
      "metadata": {
        "id": "3tkGJUI8nEDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoTokenizer, CLIPProcessor, VisionEncoderDecoderModel, ViTImageProcessor\n",
        "\n",
        "def generate_music(fine_tuned_model, image, tokenizer):\n",
        "    # Preprocess the image using the CLIP processor\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    \n",
        "    # Generate the poetry with the fine-tuned VisionEncoderDecoder model\n",
        "    generated_tokens = fine_tuned_model.generate(\n",
        "        pixel_values,\n",
        "        max_length=300,\n",
        "        num_beams=5,\n",
        "        top_p=0.9,\n",
        "        temperature=2.0,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    generated_poetry = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    return generated_poetry\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "\n",
        "path = 'AnyaSchen/image2music'\n",
        "fine_tuned_model = VisionEncoderDecoderModel.from_pretrained(path).to(device)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)"
      ],
      "metadata": {
        "id": "8_6ZpPqGwY62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://anandaindia.org/wp-content/uploads/2018/12/happy-man.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "generated_music = generate_music(fine_tuned_model, image, tokenizer)\n",
        "print(generated_music)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9cAGyM7w6gi",
        "outputId": "902491d8-e40f-4b75-8a2c-a95a99693fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:1\n",
            "L:1/4\n",
            "M:2/2\n",
            "K:F\n",
            "V:1 treble nm=\"Piano\" snmata!PMm=\"Pno\"\"C7\" c3/2 c/ c2- | c C D E |\"F\" F F F/E/F/G/ |\n",
            "\"Dm\" A2\"G7\" G2 |\"Gm7\" d d d/c/d/e/ | d2 G F |\"C\" E C\"Cm6\" D _E |\"Bb\" D F G _A |\"D7\" A c c/=B/ c |\n",
            " c c _A c |\"Eb7\" _B _e e d |\"Ab\" _d2 c2 | c c d e | f f e/d/\"Db7\"_d/=d/ |\"Gb\" c c c B | c2 G\"F6\" A |\n",
            " A c\"C+7\" ^c\"D9\" d | e e\"Ab6\" f e |\"Db9\" e d d c | d3 c/B/ | A A _A\"C9\" G | B B A G | c4 | c3 z |]\n"
          ]
        }
      ]
    }
  ]
}