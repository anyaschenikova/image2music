{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xq8dQ0glK27tCUVE7Ov_sCBftH5pBlto?usp=sharing)\n"
      ],
      "metadata": {
        "id": "DQtoXKdjJFey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('AnyaSchen/image2music_abc')"
      ],
      "metadata": {
        "id": "YyXDeQr2I15E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58229779-1c1e-470d-e4f7-a4ceab354846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/revolt/anaconda3/envs/poetry_gpt3_large/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Found cached dataset parquet (/home/revolt/.cache/huggingface/datasets/AnyaSchen___parquet/AnyaSchen--image2music_abc-784eee9f15716c2e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 441.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "bOAMKur3I7Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else None"
      ],
      "metadata": {
        "id": "b2LjPKtPI96H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "w0-QipLZJAp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c27f33-620d-4193-83dc-81fd0bafbcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quadro RTX 6000'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoTokenizer, CLIPProcessor, VisionEncoderDecoderModel, ViTImageProcessor\n",
        "\n",
        "def generate_music(fine_tuned_model, image, tokenizer):\n",
        "    # Preprocess the image using the CLIP processor\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    \n",
        "    # Generate the poetry with the fine-tuned VisionEncoderDecoder model\n",
        "    generated_tokens = fine_tuned_model.generate(\n",
        "        pixel_values,\n",
        "        max_length=300,\n",
        "        num_beams=5,\n",
        "        top_p=0.8,\n",
        "        temperature=2.0,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    generated_poetry = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    return generated_poetry\n"
      ],
      "metadata": {
        "id": "1CuYNdvrJJqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "path = 'AnyaSchen/image2music'\n",
        "fine_tuned_model = VisionEncoderDecoderModel.from_pretrained(path).to(device)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(path)\n",
        "\n",
        "# Load a GPT tokenizer for the Russian language\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "nPMvOlqSJKXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8969f0ee-2001-44cc-8d3c-ef7dd879954d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionEncoderDecoderModel(\n",
              "  (encoder): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): BartForCausalLM(\n",
              "    (model): BartDecoderWrapper(\n",
              "      (decoder): BartDecoder(\n",
              "        (embed_tokens): Embedding(50268, 768, padding_idx=1)\n",
              "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x BartDecoderLayer(\n",
              "            (self_attn): BartAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (activation_fn): GELUActivation()\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): BartAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50268, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vy_KqUXbLBZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_music = []\n",
        "for i in tqdm(range(data['train'].num_rows)):\n",
        "  generated_music.append(generate_music(fine_tuned_model, data['train'][i]['image'], tokenizer))"
      ],
      "metadata": {
        "id": "_w-TCpEfJQ3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5522d702-63db-4984-afeb-b36adaf611c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████| 1003/1003 [2:41:37<00:00,  9.67s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "\n",
        "with open('./music.pkl', 'wb') as f:\n",
        "  pickle.dump({'music': generated_music}, f)"
      ],
      "metadata": {
        "id": "cwp1G4n2Jnhw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}